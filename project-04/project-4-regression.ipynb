{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 4\n",
    "\n",
    "## San Francisco Data\n",
    "\n",
    "---\n",
    "\n",
    "[San Francisco provides a wealth of data on the city to the public.](https://data.sfgov.org/) Project 4 is all about exploring this data and modeling interesting relationships with regression.\n",
    "\n",
    "Project 4 is also much more open-ended than the previous projects. You are more responsible for finding things about the data that interest you, coming up with your own hypothesis about it, and reporting on the analysis you carry out. That being said, there are still specific requirements and restrictions on the type of models you will be constructing.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes on the data\n",
    "\n",
    "We have gone through the above website and pulled out a variety of different datasets that we think are particularly interesting. Some of the datasets are from external sources as well, but all are related to San Francisco. A high level overview of data folders is provided after the project requirements section.\n",
    "\n",
    "**There is a lot of different data. You are _NOT_ expected to explore all of it or include all of it in analyses.** The point of the project is to focus on aspects of San Francisco that are particularly interesting to you from a modeling point of view. \n",
    "\n",
    "**The uncompressed data is a large filesize.** Even the compressed data is pretty large. The data is compressed into a .7z format which has one of the smallest filesizes available. You will likely need a 3rd party app to extract it. \n",
    "\n",
    "### Recommended Utilities for .7z\n",
    "- For OSX [Keka](http://www.kekaosx.com/en/) or [The Unarchiver](http://wakaba.c3.cx/s/apps/unarchiver.html). \n",
    "- For Windows [7-zip](http://www.7-zip.org/) is the standard. \n",
    "- For Linux try the `p7zip` utility.  `sudo apt-get install p7zip`.\n",
    "\n",
    "---\n",
    "\n",
    "## Project requirements\n",
    "\n",
    "**You will be performing 4 different sections of analysis on the San Francisco data.**\n",
    "\n",
    "**Models must be regression. This means that your target variable needs to be numeric/continuous**\n",
    "\n",
    "Do not perform classification models – this will be the topic of week 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 1. Choose a dataset or datasets of interest to you and perform EDA\n",
    "\n",
    "---\n",
    "\n",
    "1. Explain what the data is. This may include multiple csv files. Some of this data has hard to understand codes representing the variables. Nearly all data is pulled from https://data.sfgov.org/ so this is a very good resource for determining what the data is.\n",
    "- Clean the data.\n",
    "- Develop and state clearly a hypothesis about the data that you would want to test.\n",
    "- Create some initial visualizations on the portions of the data relevant to your hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 2. Construct and evaluate a linear regression model on the data\n",
    "\n",
    "---\n",
    "\n",
    "1. State the variables that are predictors in your linear regression and the target variable.\n",
    "- Investigate and remove any outliers or other problems in your data. _This is a subjective process._\n",
    "- Construct a linear regression model.\n",
    "- Evaluate the model. How does the $R^2$ of the overall model compare to cross-validated $R^2$. What do the differences in $R^2$ mean?\n",
    "  - Use test / train split\n",
    "  - Use K-Folds\n",
    "  - Compare and explain your results with both\n",
    "- Visualize the evaluation metrics of your analysis in clear charts.\n",
    "- Summarize your results in the context of your hypothesis. Frame this as if you are presenting to non-technical readers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 2.2 Explain $R^2$ vs. mean squared error (MSE)\n",
    "\n",
    "---\n",
    "\n",
    "1. If you have negative $R^2$ values in cross-validation, what does this mean? \n",
    "2. Why can $R^2$ only be negative when the model is tested on new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 3. Combine multiple sources of data and evaluate a model with regularized regression\n",
    "\n",
    "---\n",
    "\n",
    "**I recommend having many predictors to see benefits from regularization methods, but it's up to you.**\n",
    "\n",
    "1. Use _NEW_ sources of data for this section. It should not be related to your analysis/hypothesis from section 1.\n",
    "- Like in part 1, you should state a hypothesis and perform data cleaning and EDA _only_ on the relevant portions of your data. Don't waste time!\n",
    "- Construct and evaluate different models with cross-validated $R^2$. Compare LinearRegression, Lasso, Ridge, and ElasticNet. Find the optimal hyperparameters (alpha, l1_ratio) using gridsearch.\n",
    "- Report on which model is best, and why that might be the case (hint: does your data have multicollinearity? Irrelevant variables? Both?)\n",
    "- Plot visuals that compare the performance of the four models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4. Conduct another, different analysis and report on it\n",
    "\n",
    "---\n",
    "\n",
    "1. Combining multiple sources of data (csv files) is required.\n",
    "- Perform EDA and cleaning on relevant data.\n",
    "- Construct and compare different regression models with cross-validation.\n",
    "- Plot descriptive visuals you think are useful for understanding the data.\n",
    "- Report on your findings.\n",
    "\n",
    "This section is just another analysis like the first sections, but is more open-ended in what models you decide to build and why. Don't feel obligated to try every out every regularized regression model (though it doesn't hurt). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4.2 Ploting GridSearch with Lasso, Ridge, or Elasticnet\n",
    "---\n",
    "1. Using the gridsearch results, plot your score in relation to your parameters.\n",
    "1. Plot the effect on your coefficients given alpha.\n",
    "1. Plot any other parameter change to visualize the impact of any other hyperparameter.\n",
    "\n",
    "It's helpful to understand the impact of parameters used in Gridsearch, and that you are comfortable working with the data that is generated for reporting to explore the results visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Bias-variance tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "1. Select San Francisco data of your choice and construct a regularized regression model (this can be data from an earlier analysis if you like). Ideally the model should actually improve across regularization strengths...\n",
    "- Gridsearch the regularization parameters to find the optimal.\n",
    "- Plot the regularization parameter against the cross-validated $R^2$.\n",
    "- Explain how regularization and regularization strength is related to the bias-variance tradeoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 5.1 Calculate the approximated $\\text{bias}^2$ and variance across regularization strengths.\n",
    "\n",
    "---\n",
    "\n",
    "You can obviously use my code from the bias-variance lab to do this. \n",
    "\n",
    "Plot the bias and variance change _with_ the cross-validated $R^2$. \n",
    "\n",
    "You'll need to scale these values somehow to put them on the same chart (I recommend (MinMaxScaler)[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html] to put $\\text{bias}^2$ and variance on the same scale as cross-validated $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/HNPKfE8.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 6. Custom regularized regression penalties\n",
    "\n",
    "---\n",
    "\n",
    "The $\\lambda1$ and $\\lambda2$ norm regularization penalties (Lasso and Ridge) are the most commonly used regularization penalties. They have a solid foundation in statistics and evidence of effectiveness. However, these are not the only possible penalties for regression – sometimes new, customized penalties give additional performance and predictive power to models depending on the context.\n",
    "\n",
    "For example, when I worked in neuroscience [we created an effective penalized regression dubbed \"GraphNet\"](http://arxiv.org/abs/1110.4139) that combined the Lasso and Ridge penalties like in Elastic net with a third penalty that rewarded clustering of coefficients according to physical distance in FMRI brain activation data.\n",
    "\n",
    "**Devise of and implement a penalized regression for San Francisco data.** What is your rationale – why would this be useful? How does it perform compared to the standard Ridge, Lasso, and Elastic Net penalties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
